{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f380ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248b7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56322f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            ms_ks=9,\n",
    "            \n",
    "    ):\n",
    "        \"\"\"\n",
    "        Argument\n",
    "            ms_ks: kernel size in message passing conv\n",
    "        \"\"\"\n",
    "        super(SCNN, self).__init__()\n",
    "        self.net_init(input_size, ms_ks)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.backbone(img)\n",
    "        x = self.layer1(x)\n",
    "        x = self.message_passing_forward(x)\n",
    "        x = self.layer2(x)\n",
    "        seg_pred = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        x = self.layer3(x)\n",
    "        x = x.view(-1, self.fc_input_feature)\n",
    "        exist_pred = self.fc(x)\n",
    "        return seg_pred, exist_pred\n",
    "\n",
    "    def message_passing_forward(self, x):\n",
    "        Vertical = [True, True, False, False]\n",
    "        Reverse = [False, True, False, True]\n",
    "        for ms_conv, v, r in zip(self.message_passing, Vertical, Reverse):\n",
    "            x = self.message_passing_once(x, ms_conv, v, r)\n",
    "        return x\n",
    "\n",
    "    def message_passing_once(self, x, conv, vertical=True, reverse=False):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        ----------\n",
    "        x: input tensor\n",
    "        vertical: vertical message passing or horizontal\n",
    "        reverse: False for up-down or left-right, True for down-up or right-left\n",
    "        \"\"\"\n",
    "        nB, C, H, W = x.shape\n",
    "        if vertical:\n",
    "            slices = [x[:, :, i:(i + 1), :] for i in range(H)]\n",
    "            dim = 2\n",
    "        else:\n",
    "            slices = [x[:, :, :, i:(i + 1)] for i in range(W)]\n",
    "            dim = 3\n",
    "        if reverse:\n",
    "            slices = slices[::-1]\n",
    "\n",
    "        out = [slices[0]]\n",
    "        for i in range(1, len(slices)):\n",
    "            out.append(slices[i] + F.relu(conv(out[i - 1])))\n",
    "        if reverse:\n",
    "            out = out[::-1]\n",
    "        return torch.cat(out, dim=dim)\n",
    "\n",
    "    def net_init(self, input_size, ms_ks):\n",
    "        input_w, input_h = input_size\n",
    "        self.fc_input_feature = 5 * int(input_w/16) * int(input_h/16)\n",
    "        self.backbone = models.vgg16_bn(pretrained=True).features\n",
    "\n",
    "        # ----------------- process backbone -----------------\n",
    "        for i in [34, 37, 40]:\n",
    "            conv = self.backbone._modules[str(i)]\n",
    "            dilated_conv = nn.Conv2d(\n",
    "                conv.in_channels, conv.out_channels, conv.kernel_size, stride=conv.stride,\n",
    "                padding=tuple(p * 2 for p in conv.padding), dilation=2, bias=(conv.bias is not None)\n",
    "            )\n",
    "            dilated_conv.load_state_dict(conv.state_dict())\n",
    "            self.backbone._modules[str(i)] = dilated_conv\n",
    "        self.backbone._modules.pop('33')\n",
    "        self.backbone._modules.pop('43')\n",
    "        # ----------------- SCNN part -----------------\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, 3, padding=4, dilation=4, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 128, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()  # (nB, 128, 36, 100)\n",
    "        )\n",
    "        # ----------------- add message passing -----------------\n",
    "        self.message_passing = nn.ModuleList()\n",
    "        self.message_passing.add_module('up_down', nn.Conv2d(128, 128, (1, ms_ks), padding=(0, ms_ks // 2), bias=False))\n",
    "        self.message_passing.add_module('down_up', nn.Conv2d(128, 128, (1, ms_ks), padding=(0, ms_ks // 2), bias=False))\n",
    "        self.message_passing.add_module('left_right',\n",
    "                                        nn.Conv2d(128, 128, (ms_ks, 1), padding=(ms_ks // 2, 0), bias=False))\n",
    "        self.message_passing.add_module('right_left',\n",
    "                                        nn.Conv2d(128, 128, (ms_ks, 1), padding=(ms_ks // 2, 0), bias=False))\n",
    "        # (nB, 128, 36, 100)\n",
    "\n",
    "        # ----------------- SCNN part -----------------\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(128, 5, 1)  # get (nB, 5, 36, 100)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Softmax(dim=1),  # (nB, 5, 36, 100)\n",
    "            nn.AvgPool2d(2, 2),  # (nB, 5, 18, 50)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_feature, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),\n",
    "            nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8afb99ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b235580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCNN((256,256),9)\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if 'backbone' in  name:\n",
    "        p.requires_grad = False\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70ea2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8        [-1, 128, 128, 128]          73,856\n",
      "       BatchNorm2d-9        [-1, 128, 128, 128]             256\n",
      "             ReLU-10        [-1, 128, 128, 128]               0\n",
      "           Conv2d-11        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-12        [-1, 128, 128, 128]             256\n",
      "             ReLU-13        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-14          [-1, 128, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 64, 64]         295,168\n",
      "      BatchNorm2d-16          [-1, 256, 64, 64]             512\n",
      "             ReLU-17          [-1, 256, 64, 64]               0\n",
      "           Conv2d-18          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-19          [-1, 256, 64, 64]             512\n",
      "             ReLU-20          [-1, 256, 64, 64]               0\n",
      "           Conv2d-21          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-22          [-1, 256, 64, 64]             512\n",
      "             ReLU-23          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25          [-1, 512, 32, 32]       1,180,160\n",
      "      BatchNorm2d-26          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-27          [-1, 512, 32, 32]               0\n",
      "           Conv2d-28          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-29          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-30          [-1, 512, 32, 32]               0\n",
      "           Conv2d-31          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-32          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-33          [-1, 512, 32, 32]               0\n",
      "           Conv2d-34          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-35          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-36          [-1, 512, 32, 32]               0\n",
      "           Conv2d-37          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-38          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-39          [-1, 512, 32, 32]               0\n",
      "           Conv2d-40          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-41          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-42          [-1, 512, 32, 32]               0\n",
      "           Conv2d-43         [-1, 1024, 32, 32]       4,718,592\n",
      "      BatchNorm2d-44         [-1, 1024, 32, 32]           2,048\n",
      "             ReLU-45         [-1, 1024, 32, 32]               0\n",
      "           Conv2d-46          [-1, 128, 32, 32]         131,072\n",
      "      BatchNorm2d-47          [-1, 128, 32, 32]             256\n",
      "             ReLU-48          [-1, 128, 32, 32]               0\n",
      "           Conv2d-49           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-50           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-51           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-52           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-53           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-54           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-55           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-56           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-57           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-58           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-59           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-60           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-61           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-62           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-63           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-64           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-65           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-66           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-67           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-68           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-69           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-70           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-71           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-72           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-73           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-74           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-75           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-76           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-77           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-78           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-79           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-80           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-81           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-82           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-83           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-84           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-85           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-86           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-87           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-88           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-89           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-90           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-91           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-92           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-93           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-94           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-95           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-96           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-97           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-98           [-1, 128, 1, 32]         147,456\n",
      "           Conv2d-99           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-100           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-101           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-102           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-103           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-104           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-105           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-106           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-107           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-108           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-109           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-110           [-1, 128, 1, 32]         147,456\n",
      "          Conv2d-111           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-112           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-113           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-114           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-115           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-116           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-117           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-118           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-119           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-120           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-121           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-122           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-123           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-124           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-125           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-126           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-127           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-128           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-129           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-130           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-131           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-132           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-133           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-134           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-135           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-136           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-137           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-138           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-139           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-140           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-141           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-142           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-143           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-144           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-145           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-146           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-147           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-148           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-149           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-150           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-151           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-152           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-153           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-154           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-155           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-156           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-157           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-158           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-159           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-160           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-161           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-162           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-163           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-164           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-165           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-166           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-167           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-168           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-169           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-170           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-171           [-1, 128, 32, 1]         147,456\n",
      "          Conv2d-172           [-1, 128, 32, 1]         147,456\n",
      "       Dropout2d-173          [-1, 128, 32, 32]               0\n",
      "          Conv2d-174            [-1, 5, 32, 32]             645\n",
      "         Softmax-175            [-1, 5, 32, 32]               0\n",
      "       AvgPool2d-176            [-1, 5, 16, 16]               0\n",
      "          Linear-177                  [-1, 128]         163,968\n",
      "            ReLU-178                  [-1, 128]               0\n",
      "          Linear-179                    [-1, 4]             516\n",
      "         Sigmoid-180                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 38,024,777\n",
      "Trainable params: 23,301,641\n",
      "Non-trainable params: 14,723,136\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 477.96\n",
      "Params size (MB): 145.05\n",
      "Estimated Total Size (MB): 623.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bcd2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "ce_loss = nn.CrossEntropyLoss(weight=torch.tensor([0.4, 1, 1, 1, 1]))\n",
    "bce_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb92f971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
